---
layout: narrative
title: Variational Machine Translation
author: Anshul Samar
date: 2018-01-04
mydate: Jan 2018
---

<script
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
type="text/javascript"></script>

In August 2015, I gave a tutorial talk to the Stanford Deep Learning
Reading group on Inverse Graphics Networks, an application of
Variational Auto-Encoding. This post is part walk-through of Kingma
and Welling's Variational Auto-Encoding paper, part overview of
Inverse Graphics Networks, and part discussion of my own proposed variational
machine translation algorithm. It is based on written notes from
that tutorial. 

To see my contribution, please jump to the last section.

[Introduction](#Introduction)  
[Variational Bayes](#VariationalBayes)  
[Reparameterization](#Reparameterization)  
[Putting it Together](#PuttingItTogether)  
[Inverse Graphics Networks](#InverseGraphicsNetworks)  
[Variational Machine Translation](#VariationalMachineTranslation)  

<a name="Introduction"></a> **Introduction**

Say we believe that our dataset is
the result of sampling an underlying unknown distribution. For example,
a dataset of a cars might be the result of an underlying probability
distribution that favors small cheap cars over luxury vehicles and the
colors white and silver. While the purchase
of each individual car may be a complex process involving many
variables, we try to abstractly model every data point as a sample
from an underlying distribution. 

Let \\(X = {x^{(i)}}_{i=1}^N \\) be our dataset. We assume that \\(
x^{(i)} \\) is generated by:

1. Sampling \\( z^{(i)} \sim P_\theta(z)\\) from the prior.
2. Sampling \\( x^{(i)} \sim P_\theta(x\|z)\\) from the likelihood.

What questions can we ask?

1. Which underlying \\(P\\) distribution should we use (i.e. gaussian)
and which parameters \\(\theta\\) best describe this dataset?
(i.e. what are the best \\(\mu\\) and \\(\sigma\\)?). Knowing this
will help us generate new data. 
3. What is the posterior distribution \\(P_{\theta}(z\|x)\\)? This is
useful for determining the underlying "representation" of a data point.
4. What is the marginal likelihood \\(P_\theta(x)\\)? This is useful
for classifying new points and determining how likely a data point is.

Say we knew which hidden variable corresponded to which
\\(x^{(i)}\\). Then we can determine \\(P_\theta(x,z) =
P_\theta(x\|z)P_\theta(z)\\) for every \\(x\\). The likelihood of our
dataset would be \\(\prod_i
P_\theta(x^{(i)}\|z^{(i)})P_\theta(z^{(i)})\\). The log likelihood is \\(\sum_i log(P_\theta(x^{(i)}\|z^{(i)})) +
log(P_\theta(z^{(i)})) \\). We would set this equal to zero, take
derivatives, and determine the best parameter \\(\theta\\) to maximize the likelihood of the data.

Unfortunately, we do not know which hidden variable corresponds to
which data point. Now, our log likelihood is \\(\sum_i log (\int
P_\theta(x|z) P_\theta(z) dz)\\). Log of sums becomes difficult to work
with. See mixture of gaussian notes below for an example of this.

While one may now suggest using the EM algorithm and the posterior
distribution to iteratively estimate \\(\theta\\), what if we wish to use
likelihoods and posteriors that are intractable (i.e. no closed form
or too computationally expensive)? Furthermore, what if with our large
datasets, sampling is infeasible? As Kingma and Welling
write, "these intractabilities are quite common and appear in cases of
moderately complicated likelihood functions \\(p_\theta(x|z)\\), e.g. a neural
network with a nonlinear hidden layer" [1]. 

For example, in order to determine the marginal likelihood of x, we
need to sweep over all possible values for \\(z\\), i.e.: \\(P_\theta(x) = \int P_\theta(z)P_\theta(x\|z)dz \\). If the likelihood is given to us by a
neural network, this becomes very difficult to determine (as a simple
exercise, imagine likelihood given by a two sigmoid units connected to
each other). The posterior \\(P_\theta(z\|x) =
\frac{P_\theta(x\|z)P_\theta(z)}{P_\theta(x)}\\) becomes similarly
difficult to compute.

To summarize: we are interested in \\(P_\theta(x)\\) and \\(P_\theta(z|x)\\), as
well as the parameters for our underlying distribution. All
we know is our original dataset \\(X\\). Traditional methods are
intractable. What can we do?

<a name="VariationalBayes"></a> **Variational Bayes**

Let's try to find a function \\(q_\phi(z|x)\\) that is very close to
our posterior \\(P_\theta(z|x)\\). Because both \\(q\\) and \\(p\\)
are probability distributions, we use the KL Divergence as cost. 

$$
\begin{align*}
D_{KL}(q_\phi(z|x) || p_\theta(z|x)) &= E_{q_\phi(z|x)}[log
\frac{q_\phi(z|x)}{p_\theta(z|x)}] \\
&= E_{q_\phi(z|x)}[log q_\phi(z|x)] - E_{q_\phi(z|x)}[log
P_\theta(z,x)] + E_{q_\phi(z|x)}[log P_\theta(x)] \\
&=-(E_{q_\phi(z|x)}[logP_\theta(z,x)] - E_{q_\phi(z|x)}[log
q_\phi(z|x)])) + log P_\theta(x)
\end{align*}
$$

Step 1 is the definition of KL divergence on the respective
probability distributions. To go to step 2, we distribute the log,
using bayes formula to replace the posterior with the joint
probability and prior. To go to step 3, we regroup and use the fact
that the prior under P is constant with respect to q (thus, removing
the expectation).

The first term in line 3 is called the ELBO: evidence based lower
bound (as it is based purely on things we know - the "evidence" and
our approximate distribution). 

$$E_{q_\phi(z|x)}[logP_\theta(z,x)] - E_{q_\phi(z|x)}[log
q_\phi(z|x)])$$

What is it the lower bound to?

Note that:

$$
\begin{align*}
log P_\theta(x) &= log \int_z P_\theta(x,z) \\
&= log \int_z P_\theta(x,z) \frac{q_\phi(z|x)}{q_\phi(z|x)} \\
&= log E_{q_\phi(z|x)}[ \frac{P_\theta(x,z)}{q_\phi(z|x)}] \\
&\geq E_{q_\phi(z|x)}[log p_\theta(x,z)] - E_{q_\phi(z|x)}[log q(z|x)] \\
&= ELBO
\end{align*}
$$

This is also called the variational lower bound. We can rewrite the
ELBO in another formulation. Now onwards, I use shorthand \\(q_\phi\\)
to denoate \\(q_\phi(z|x)\\). 

$$
\begin{align*}
ELBO(\theta, \phi; x^{(i)}) &= E_{q_\phi}[-log q_\phi + log
p_\theta(x,z)] \\
&= -E_{q_\phi}[log q_\phi - log p_\theta(z) - log p_\theta(x|z)] \\
&= -D_{KL}(q_\phi || p_\theta(z)) + E_{q_\phi}(P_\theta(x|z))
\end{align*}
$$

Step 2 is by spliting the joint distribution into individual
components.

Note that by maximizing ELBO, we seek parameters \\(\theta, \phi\\)
that lower \\(D_{KL}(q_\phi || p_\theta(z))\\), while increasing
E_{q_\phi}(P_\theta(x|z)). Why is this interesting? Note that the KL
Divergence term ensures that our approximate posterior is close to our
prior over \\(z\\) (say gaussian). The second term ensures that 








**References**

Because the talk happened some time ago and I am writing the post
now based on written notes, I don't remember the references I used outside
of the two main papers below. I had taken some bits and insights freely from wikipedia,
blogs, and peers, as this was presented in an informal setting, so
some language may overlap. Thanks
to Ziang Xie, Jonathan Ho, and Kenneth Jung for helpful conversations.


<a href="https://arxiv.org/pdf/1312.6114.pdf">[1] </a>Diederik Kingma and Max Welling. *Auto-Encoding Variational Bayes*. The
2nd International Conference on Learning Representations (ICLR). 2013.

<a href="https://arxiv.org/abs/1503.03167">[2] </a> Tejas Kulkarni,
William Whitney, Pushmet Kohli, and Joshua Tenenbaum. *Deep Convolutional Inverse Graphics Network.* NIPS. 2015.

<a
href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/">[3]
</a> Agustinus Kristiadi. "MLE vs MAP." wiseodd.github.io. 2017. 

<a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">[4] </a>
Andrew Ng. "The EM Algorithm." cs229. 

<a href="http://cs229.stanford.edu/notes/cs229-notes7b.pdf">[5] </a>
Andrew Ng. "Mixtures of Gaussians." cs229.



